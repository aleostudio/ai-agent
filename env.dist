# App config (127.0.0.1 for localhost, 0.0.0.0 for whole network)
APP_NAME="Simple AI agent"
APP_HOST=0.0.0.0
APP_PORT=9201
API_VERSION="v1"

# Logging
DEBUG=False

# Provider config (host.docker.internal for Ollama on Docker)
#
# Provider | Provider base url
# ---------+------------------
# ollama   | http://localhost:11434
# openai   | https://api.openai.com/v1
# groq     | https://api.groq.com
PROVIDER="ollama"
PROVIDER_BASE_URL="http://localhost:11434"
PROVIDER_API_KEY="-"

# Model config
MODEL="llama3.2:latest"
TEMPERATURE=0.5
MAX_RETRIES=3

# Response config
# full -> Full raw model response
# text -> Textual response only
RESPONSE_TYPE="full"

# Agent config
SOME_USEFUL_VAR="some_value"
