# App config (127.0.0.1 for localhost, 0.0.0.0 for whole network)
APP_NAME="Simple AI agent"
APP_HOST=0.0.0.0
APP_PORT=9201
API_VERSION="v1"

# Logging
DEBUG=False

# Provider config (host.docker.internal for Ollama on Docker)
#
# Provider | Provider base url
# ---------+------------------
# ollama   | http://localhost:11434
# openai   | https://api.openai.com/v1
# groq     | https://api.groq.com
PROVIDER="ollama"
PROVIDER_BASE_URL="http://localhost:11434"
PROVIDER_API_KEY="-"

# Model config
MODEL="llama3.2:latest"
TEMPERATURE=0.5
MAX_RETRIES=3

# Response config (full: raw model response; text: textual response only)
RESPONSE_TYPE="full"

# MCP servers
MCP_ENABLED=False
MCP_SERVERS='[{"name": "mcp-server", "transport": "sse", "url": "http://localhost:8000/sse"}]'
MCP_TOOL_CALL_MAX_ITERATIONS=10
